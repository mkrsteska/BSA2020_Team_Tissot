{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Video\" data-toc-modified-id=\"Video-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Video</a></span><ul class=\"toc-item\"><li><span><a href=\"#GitHub\" data-toc-modified-id=\"GitHub-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>GitHub</a></span></li></ul></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Sentiment Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Approaches\" data-toc-modified-id=\"Approaches-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Approaches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Rule-based-system\" data-toc-modified-id=\"Rule-based-system-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Rule-based system</a></span></li><li><span><a href=\"#Automatic-systems\" data-toc-modified-id=\"Automatic-systems-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Automatic systems</a></span></li><li><span><a href=\"#Hybrid-systems\" data-toc-modified-id=\"Hybrid-systems-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Hybrid systems</a></span></li></ul></li></ul></li><li><span><a href=\"#Applications\" data-toc-modified-id=\"Applications-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Applications</a></span><ul class=\"toc-item\"><li><span><a href=\"#Social-Media-Monitoring\" data-toc-modified-id=\"Social-Media-Monitoring-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Social Media Monitoring</a></span></li><li><span><a href=\"#Brand-Monitoring\" data-toc-modified-id=\"Brand-Monitoring-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Brand Monitoring</a></span></li><li><span><a href=\"#Voice-of-customer\" data-toc-modified-id=\"Voice-of-customer-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Voice of customer</a></span></li><li><span><a href=\"#Market-research\" data-toc-modified-id=\"Market-research-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Market research</a></span></li></ul></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Cloud-AutoML\" data-toc-modified-id=\"Cloud-AutoML-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Cloud AutoML</a></span><ul class=\"toc-item\"><li><span><a href=\"#Natural-Language-API-by-Google\" data-toc-modified-id=\"Natural-Language-API-by-Google-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Natural Language API by Google</a></span></li><li><span><a href=\"#Google-AutoML-Natural-Language\" data-toc-modified-id=\"Google-AutoML-Natural-Language-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Google AutoML Natural Language</a></span><ul class=\"toc-item\"><li><span><a href=\"#Features-of-Google-AutoML-Natural-Language\" data-toc-modified-id=\"Features-of-Google-AutoML-Natural-Language-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Features of Google AutoML Natural Language</a></span></li></ul></li></ul></li><li><span><a href=\"#Custom-Sentiment-Analysis-model-using-Google-AutoML-Natural-Language\" data-toc-modified-id=\"Custom-Sentiment-Analysis-model-using-Google-AutoML-Natural-Language-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Custom Sentiment Analysis model using Google AutoML Natural Language</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-a-project\" data-toc-modified-id=\"Set-up-a-project-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Set up a project</a></span></li><li><span><a href=\"#Enable-the-APIs\" data-toc-modified-id=\"Enable-the-APIs-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Enable the APIs</a></span></li><li><span><a href=\"#Prepare-the-dataset\" data-toc-modified-id=\"Prepare-the-dataset-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Prepare the dataset</a></span></li><li><span><a href=\"#Create-and-import-the-dataset\" data-toc-modified-id=\"Create-and-import-the-dataset-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Create and import the dataset</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Evaluate-the-model\" data-toc-modified-id=\"Evaluate-the-model-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Evaluate the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Confusion-matrix\" data-toc-modified-id=\"Confusion-matrix-7.6.1\"><span class=\"toc-item-num\">7.6.1&nbsp;&nbsp;</span>Confusion matrix</a></span></li><li><span><a href=\"#Precision-and-Recall\" data-toc-modified-id=\"Precision-and-Recall-7.6.2\"><span class=\"toc-item-num\">7.6.2&nbsp;&nbsp;</span>Precision and Recall</a></span></li></ul></li><li><span><a href=\"#Test-and-use-the-model\" data-toc-modified-id=\"Test-and-use-the-model-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Test and use the model</a></span></li><li><span><a href=\"#Unbalanced-dataset-vs.-balanced-dataset\" data-toc-modified-id=\"Unbalanced-dataset-vs.-balanced-dataset-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Unbalanced dataset vs. balanced dataset</a></span></li></ul></li><li><span><a href=\"#Sentiment-Analysis-using-Python\" data-toc-modified-id=\"Sentiment-Analysis-using-Python-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Sentiment Analysis using Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-the-project\" data-toc-modified-id=\"Set-up-the-project-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Set-up the project</a></span></li><li><span><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Evaluate-the-model\" data-toc-modified-id=\"Evaluate-the-model-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Evaluate the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Precision-and-Recall\" data-toc-modified-id=\"Precision-and-Recall-8.4.1\"><span class=\"toc-item-num\">8.4.1&nbsp;&nbsp;</span>Precision and Recall</a></span></li><li><span><a href=\"#Confustion-Matrix\" data-toc-modified-id=\"Confustion-Matrix-8.4.2\"><span class=\"toc-item-num\">8.4.2&nbsp;&nbsp;</span>Confustion Matrix</a></span></li></ul></li></ul></li><li><span><a href=\"#Comparison-of-the-models\" data-toc-modified-id=\"Comparison-of-the-models-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Comparison of the models</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Bibliography\" data-toc-modified-id=\"Bibliography-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Bibliography</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A video tutorial that briefly explains what Sentiment Analysis is and walks you through the process of creating a custom Sentiment Analysis model using Google AutoML Natural Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/ZHC-kOKHU9E\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f27fa3144a8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('ZHC-kOKHU9E', width=700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://github.com/mkrsteska/BSA2020_Team_Tissot) to the GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past decade, we have seen an outstanding growth in the number of users on social media sites. Today, we\n",
    "can observe billions of messages being exchanged daily on sites like Twitter, Facebook, Instagram, etc. These messages are rich source of information about people’s opinions and feelings, as well as public sentiment towards\n",
    "certain people, news, events and various topics. \n",
    "\n",
    "Automatic tools for real-time analysis of this sentiment can be very beneficial for governments, businesses, public people, and many other actors who would benefit from a real-time global feedback of their words and actions.\n",
    "\n",
    "For this reason, there has been a huge interest in both academia and industry for using social media data in analyzing public sentiment. \n",
    "\n",
    "In this notebook, we will give a detailed explanation how to build a custom sentiment analysis model using Google AutoML and we demonstrate how it can be used in a domain-specific fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand sentiment analysis, a first notion must be introduced, text mining. Text mining is the process of transforming unstructured text data into meaningful and actionable information. \n",
    "\n",
    "**Sentiment Analysis, also known as opinion mining or emotion AI, is contextual mining of text to determine the emotional tone behind a series of words, used to gain an understanding of the expressed attitudes, opinions and emotions**.\n",
    "\n",
    "A sentiment analysis system for text analysis combines natural language processing (NLP) and machine learning techniques to assign weighted sentiment scores to the entities, topics, themes and categories within a sentence or phrase. \n",
    "\n",
    "So, we can say it is **the process of determining whether a piece of writing is positive, negative or neutral**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human language is complex. Teaching a machine to analyse the various grammatical nuances, cultural variations, slang and misspellings that occur in text is a difficult process. Teaching a machine to understand how context can affect tone is even more difficult.\n",
    "\n",
    "Consider the following sentence: 'My flight has been cancelled. Brilliant!'. \n",
    "\n",
    "Most humans would be able to quickly interpret that the person was being sarcastic. We know that for most people having a cancelled flight is not a good experience. By applying this contextual understanding to the sentence, we can easily identify the sentiment as negative. Without contextual understanding, a machine looking at the sentence above might see the world “brilliant” and categorise it as positive.\n",
    "\n",
    "In this notebook, we will explore several approaches that tackle this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three main approaches for sentiment analysis are: \n",
    "\n",
    "**Rule-based systems** that perform sentiment analysis based on a set of manually crafted rules. \n",
    "<br>\n",
    "**Automatic systems** that rely on machine learning techniques to learn from data.\n",
    "<br>\n",
    "**Hybrid systems** that combine both rule-based and automatic approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, a rule-based system uses a set of human-crafted rules to help identify subjectivity, polarity, or the subject of an opinion.\n",
    "<br>\n",
    "These rules may include various techniques developed in computational linguistics, such as:\n",
    "- Stemming, tokenization, part-of-speech tagging and parsing.\n",
    "- Lexicons (i.e. lists of words and expressions).\n",
    "\n",
    "Here’s a basic example of how a simple rule-based system works:\n",
    "1. Defines two lists of polarized words (e.g. negative words such as bad, worst, ugly, etc and positive words such as good, best, beautiful, etc).\n",
    "2. Counts the number of positive and negative words that appear in a given text.\n",
    "3. If the number of positive word appearances is greater than the number of negative word appearances, the system returns a positive sentiment, and vice versa. If the numbers are even, the system will return a neutral sentiment.\n",
    "<br>\n",
    "\n",
    "\n",
    "Rule-based systems are very naive since they don't take into account how words are combined in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic methods, contrary to rule-based systems, don't rely on manually crafted rules, but on machine learning techniques. A sentiment analysis task is usually modeled as a classification problem, whereby a classifier is fed a text sequence and returns a category, e.g. positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid systems combine the desirable elements of rule-based and automatic techniques into one system, so they make up for the deficiencies of each approach.\n",
    "\n",
    "Hybrid sentiment analysis systems combine natural language processing with machine learning to identify weighted sentiment phrases within their larger context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will list where sentiment analysis can be applid and we will give some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using sentiment analysis on social media, businesses can get incredible insights into the quality of conversation that’s happening around a brand.\n",
    "\n",
    "Sentiment analysis can be used to:\n",
    "- Analyze tweets and facebook posts over a period of time to detect sentiment of a particular audience\n",
    "- Monitor social media mentions of your brand and automatically categorize by urgency\n",
    "- Gain deep insights into what’s happening across your social media channels\n",
    "\n",
    "Sentiment analysis helps you:\n",
    "- Prioritize action. Which is more urgent: a fuming customer or a “thanks!” shout-out? Obviously the angry customer. Sentiment analysis lets you easily filter unread mentions by positivity and negativity, helping you prioritize issues.\n",
    "- Track trends over time.\n",
    "- Monitor your competitors’ social media the same way you monitor your own. If you tune in closely, maybe you notice there’s been a negative response to a particular feature of their new product, and you respond by designing a lead generation campaign targeting exactly that gap. They won’t even know what hit them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do brands have a wealth of information available on social media, but also across the internet. Instead of focusing on specific social media platforms such as Facebook and Twitter, we can find mentions in places like news, blogs, and forums.\n",
    "\n",
    "Sentiment analysis can be used to: \n",
    "- Analyze news articles, blog posts, forum discussions, and other texts on the internet over a period of time to see sentiment of a particular audience\n",
    "- Automatically categorize the urgency of all online mentions of your brand\n",
    "- Better understand a brand online presence by getting all kinds of interesting insights and analytics\n",
    "\n",
    "Sentiment analysis helps you:\n",
    "- Understand how your brand reputation evolves over time\n",
    "- Research your competition and understand how their reputation also evolves over time.\n",
    "\n",
    "**Example: Expedia Canada**\n",
    "\n",
    "Around Christmas time, Expedia Canada ran a classic “escape winter” marketing campaign. All was well, except for the screeching violin they chose as background music. Understandably, people took to social media, blogs, and forums. Expedia noticed right away and removed the ad. Then, they created a series of follow-up spin-off videos: one showed the original actor smashing the violin, and in another one, they invited a real follower who had complained on Twitter to come in and rip the violin out of the actor’s hands. Though their original campaign was a flop, Expedia were able to redeem themselves by listening to their customers and responding.\n",
    "\n",
    "**Using sentiment analysis (and machine learning), you can automatically monitor all chatter around your brand and detect this type of potentially-explosive scenario while you still have time to defuse it**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice of customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social media and brand monitoring offer immediate, unfiltered, invaluable information on customer sentiment. However, there are two more insights – surveys and customer support interactions.\n",
    "\n",
    "Net Promoter Score (NPS) surveys are one of the most popular ways for businesses to gain feedback, and start by asking a simple question – Would you recommend this company, product, and/or service to a friend or family member? – that results in a simple number or score. \n",
    "\n",
    "Businesses use these scores to identify customers as promoters, passives, or detractors. The goal is to identify overall customer experience, and find ways to elevate all customers to “promoter” level, where they theoretically will buy more, stay longer, and refer other customers.\n",
    "\n",
    "Numerical survey data is easily aggregated and assessed, but the next question in NPS surveys asks customers why they left the score they did. This triggers a series of open-ended responses that are a lot harder to analyze. However, with sentiment analysis these texts can be classified into positive and negative giving you further insights into why customers left the scores they did.\n",
    "\n",
    "Sentiment analysis can be used to:\n",
    "- Analyze aggregated NPS or other survey responses\n",
    "- Analyze aggregated customer support interactions\n",
    "- Target individuals to improve their service. By automatically running sentiment analysis on incoming surveys, you can detect customers who are ‘strongly negatively’ towards your product or service, so you can respond to them right away\n",
    "\n",
    "**Example: McKinsey City Voices project**\n",
    "\n",
    "In Brazil, federal public spending rose by 156% from 2007 to 2015 while people’s satisfaction with public services steadily decreased. Unhappy with this counterproductive progress, the Urban-planning Department recruited McKinsey to help them work on a series of new projects that would focus first on user experience, or citizen journeys, when delivering services. This citizen-centric style of governance has led to the rise of what we call Smart Cities.\n",
    "\n",
    "McKinsey developed a tool called City Voices, which conducts citizen surveys across more than 150 different metrics, and then runs sentiment analysis to help leaders understand how constituents live and what they need, in order to better inform public policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a final use case, sentiment analysis empowers all kinds of market research and competitive analysis. Whether you’re exploring a new market, anticipating future trends, or having an edge over the competition, sentiment analysis can make all the difference.\n",
    "\n",
    "Sentiment analysis can be used to:\n",
    "- Analyze product reviews of your brand and compare those with the competition\n",
    "- Generate weekly, monthly, or daily reports – a sort of early-warning system\n",
    "- Compare sentiment across international markets\n",
    "- Analyze tweets and social media posts for real-time happenings\n",
    "- Analyze reviews for unfiltered customer feedback\n",
    "\n",
    "Sentiment analysis helps you:\n",
    "- Tap into new sources of information\n",
    "- Quantify otherwise qualitative information\n",
    "- Provide information in real-time rather than in retrospect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a model that identifies the sentiment of tweets about U.S. airlines using a dataset from FigureEight. Twitter data was scraped from February of 2015 and contributors were asked to classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as “late flight” or “rude service”).\n",
    "\n",
    "You can download the dataset in a CSV format directly from [here](https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/Airline-Sentiment-2-w-AA.csv\", encoding='latin-1')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Dimensionality of the DataFrame**\n",
    "<br>\n",
    "The dataset has 14640 rows and 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'airline_sentiment',\n",
       "       'airline_sentiment:confidence', 'negativereason',\n",
       "       'negativereason:confidence', 'airline', 'airline_sentiment_gold',\n",
       "       'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord',\n",
       "       'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are:\n",
    "- _unit_id\n",
    "- _golden\n",
    "- _unit_state\n",
    "- _trusted_judgments\n",
    "- _last_judgment_at\n",
    "- airline_sentiment\n",
    "- airline_sentiment:confidence\n",
    "- negativereason\n",
    "- negativereason:confidence\n",
    "- airline\n",
    "- airline_sentiment_gold\n",
    "- name\n",
    "- negativereason_gold\n",
    "- retweet_count\n",
    "- text\n",
    "- tweet_coord\n",
    "- tweet_created\n",
    "- tweet_id\n",
    "- tweet_location\n",
    "- user_timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Keep the relevant features - text and airline_sentiment** <br>\n",
    "We will only consider the text as a feature and the sentiment as a target, because we want to focus on sentiment analysis of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Remove duplicate text**\n",
    "<br>\n",
    "First we remove the extra white space at the beginning and at the end of the text, then we drop the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.strip()\n",
    "df = df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Convert airline_sentiment to numeric value**\n",
    "    - NEGATIVE sentiment to 0 \n",
    "    - NEUTRAL sentiment to 1\n",
    "    - POSITIVE sentiment to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('negative', 0) \n",
    "df = df.replace('neutral', 1)\n",
    "df = df.replace('positive', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Check distribution of classes**\n",
    "<br>\n",
    "We can see that the dataset is not equally distributed among the three possible classes - positive, neutral and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "0    9080\n",
       "1    3057\n",
       "2    2290\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['airline_sentiment'])['airline_sentiment'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Create a balanced dataset**\n",
    "<br>\n",
    "Each class will have 2290 samples, which is the maximum number of samples for positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df.groupby('airline_sentiment').apply(lambda x: x.sample(2290)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Write the balanced dataset to a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_csv(\"../data/airline_dataset_balanced.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, our dataset consists of 6870 tweets, equally distributed among three possible classes - positive, neutral and negative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also save the unabalanced dataset for later use and comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/airline_dataset_unbalanced.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud AutoML is a suite of machine learning products that enables developers with limited machine learning expertise to train high-quality models specific to their business needs. It relies on Google’s state-of-the-art transfer learning and neural architecture search technology.\n",
    "\n",
    "One of the available AutoML products is **AutoML Natural Language**, which enables you to build and deploy custom machine learning models that analyze documents, categorize documents, identify entities or assess attitudes within documents.\n",
    "\n",
    "However, if you don't need a custom model solution, the Cloud **Natural Language API** provides natural language understanding technologies to developers, including sentiment analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language API by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try the Natural Langugae API by Google on this [link](https://cloud.google.com/natural-language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language API is a pretrained machine learning model that can analyze syntax, extract entities, and evaluate the sentiment of text.\n",
    "\n",
    "So, it can tell you the sentiment of a block of text, like whether the tweet is mostly positive or mostly negative. The score range is from -1, very negative, to +1 very positive.\n",
    "\n",
    "On the image below you can see the sentiment score of the default example given by Google, it is 0.3 for the entire document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what about for text with domain-specific language?\n",
    "\n",
    "For example, lets say we are trying to classify the sentiment of a tweet like: **@AmericanAir, I was waiting on the tarmac for two hours!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An out of the box sentiment model will tell us that this tweet is neutral, as we can see on the image above. It doesn't realize that being stuck on the runway is a bad thing. \n",
    "\n",
    "For this reason, we will use AutoML Natural Language to build a custom sentiment analysis model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google AutoML Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the image below you can see the overview of the process how Google AutoML works. The first step is to upload label text, that is based on your domain-specific keywords and phrases. Second, you train your model. At the end you evaluate it and get valuable insights that are relevant to your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Google AutoML Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML Natural Language uses machine learning to analyze the structure and meaning of documents. You train a custom machine learning model to classify documents, extract information, or understand the sentiment of authors.\n",
    "\n",
    "The features are:\n",
    "\n",
    "- **Integrated REST API** \n",
    "<br> Natural Language is accessible via our REST API. Text can be uploaded in the request or integrated with Cloud Storage. <br>\n",
    "\n",
    "\n",
    "- **Custom entity extraction**\n",
    "<br> Identify entities within documents and label them based on your own domain-specific keywords or phrases.<br>\n",
    "\n",
    "- **Custom sentiment analysis**\n",
    "<br> Understand the overall opinion, feeling, or attitude expressed in a block of text tuned to your own domain-specific sentiment scores.<br>\n",
    "\n",
    "\n",
    "- **Custom content classification**\n",
    "<br> Create labels to customize models for unique use cases, using your own training data.<br>\n",
    "\n",
    "\n",
    "- **Custom models**\n",
    "<br>Train custom machine learning models with minimum effort and machine learning expertise.<br>\n",
    "\n",
    "\n",
    "- **Powered by Google’s AutoML models**\n",
    "<br> Leverages Google state-of-the-art AutoML technology to produce high-quality models.<br>\n",
    "\n",
    "\n",
    "- **Spatial structure understanding**\n",
    "<br> Use the structure and layout information in PDFs to improve custom entity extraction performance.<br>\n",
    "\n",
    "\n",
    "- **Large dataset support**\n",
    "<br> Unlock complex use cases with support for 5,000 classification labels, 1 million documents, and 10 MB document size.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Sentiment Analysis model using Google AutoML Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will give a detailed explanation how to build a custom sentiment analysis model using Google AutoML Natural Language, following the [quickstart guide](https://cloud.google.com/natural-language/automl/docs/quickstart) provided by Google. It shows you how to use AutoML Natural Language to create a custom machine learning model. You can create a model to classify documents, identify entities in documents, or analyze the prevailing emotional attitude in a document. \n",
    "\n",
    "First thing you need to do is [sign in](https://console.cloud.google.com/) to Google Cloud Platform using a Google Account. You are required to enter payment details, but you won't be charged. You get $300 for free and you can use them in a period of one year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Cloud Console, on the project selector page, [create a Cloud project](https://console.cloud.google.com/projectselector2/home/dashboard?_ga=2.255846286.724484901.1583615439-1508727016.1582035723).\n",
    "\n",
    "The following images will guide you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable the APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable the Cloud AutoML and Storage APIs on this [link](https://console.cloud.google.com/flows/enableapi?apiid=storage-component.googleapis.com,automl.googleapis.com,storage-api.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.263244682.724484901.1583615439-1508727016.1582035723).\n",
    "\n",
    "The images below will guide you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already prepared the dataset in Section 5 and we exported it in a CSV file named '**airline_dataset_balanced.csv**'.  \n",
    "\n",
    "\n",
    "**When preparing a dataset for building a custom sentiment analysis model using Google AutoML, there are a few important things to keep in mind**:\n",
    "- The CSV file should have only two columns. \n",
    "<br> left column - text and right column - sentiment score. <br>\n",
    "\n",
    "`dataframe = dataframe[['text', 'airline_sentiment']]`\n",
    "\n",
    "\n",
    "- The CSV file should not contain duplicate content in the text column. <br>\n",
    "**Important note**: The same text with extra white space at the beginning or at the end of the text is considered duplicate content as well. Make sure to strip the text before droping the duplicates. <br>\n",
    "\n",
    "`dataframe['text'] = dataframe['text'].str.strip()` <br>\n",
    "`dataframe = dataframe.drop_duplicates(subset=['text'])`\n",
    "\n",
    "\n",
    "- The CSV file should not contain indexing of the rows and a header. \n",
    "\n",
    "`dataframe.to_csv(\"airline_dataset_unbalanced.csv\", **index=False**, **header=False**)` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Open the [AutoML Natural Language UI](https://console.cloud.google.com/natural-language?_ga=2.68756631.724484901.1583615439-1508727016.1582035723) and select 'Get started'** in the AutoML Sentiment Analysis box, in order to start building a machine learning model to analyze attitudes within text and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Click the 'New Dataset' button in the title bar.**\n",
    "\n",
    "\n",
    "3. **Enter a name for the dataset.** \n",
    "\n",
    "\n",
    "4. **Leave the location set to 'Global'.**\n",
    "\n",
    "\n",
    "5. **Select the model objective 'Sentiment Analysis'.** \n",
    "\n",
    "\n",
    "6. **Set Sentiment scale**\n",
    "<br>For sentiment scale, we will tell AutoML the range of sentiment scores to expect in the dataset. Since our scores go from 0, the most negative, to 2, the most positive, we will select 2 as the maximum sentiment score. \n",
    "\n",
    "\n",
    "7. **Click 'Create Dataset'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Select files to import** <br>\n",
    "You can directly upload a CSV file from your computer. Upload the balanced airline dataset we just created and choose a bucket - destination on your Cloud Storage. (I just created one randomly :))\n",
    "\n",
    "\n",
    "9. **Click import.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Processing text items** \n",
    "<br> The data is importing and the text items are being porcessed. This can take several minutes or more. You will be emailed once importing has completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is imported, in the 'Items' tab you will see all the text items listed. \n",
    "\n",
    "The navigation bar along the left summarizes the number of labeled and unlabeled text and enables you to **filter the text list by label** (sentiment score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can click an example and make sure it has the right sentiment score. You can change it manually if it doesn't.\n",
    "\n",
    "This is shown on the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin training the model.\n",
    "\n",
    "\n",
    "1. **Navigate over to the 'Train' tab**\n",
    "\n",
    "\n",
    "2. **Click 'Start training'**\n",
    "\n",
    "Training a model can take several hours. You will be emailed once the training is done.  \n",
    "\n",
    "The images below show the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training is finished, you will be able to see you model in the 'Train' tab. \n",
    "\n",
    "You can see how well it did by clicking 'See full evaluation'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix tells us how the real labels of the dataset compare to the labels that our model predicted. \n",
    "\n",
    "The confusion matrix for this model is shown below.\n",
    "\n",
    "For example, 82% of the time, the model accurately predicted that tweets with the label 0, should have had label 0, but 17% of the time, it also thought that tweets with label 1, those that are just neutral, should be labeled 0. \n",
    "\n",
    "Additionally, we can see the overall precision and recall of the model. A high precision model produces fewer false positives. The precision of our model is 79.91%, which is the average value of the accurately predicted labels from the confusion matrix (82%, 75%, 83%). Recall, on the other hand, is equal to precision for overall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we filter a single sentiment label, we can see the precision and recall broken down for each category. \n",
    "\n",
    "If we select 'Sentiment score 0':\n",
    "- **Precision tells us of tweets that our model labeled  0, how many really were 0?**\n",
    "\n",
    "- **Recall, on the other hand, tells us of all the tweets in the dataset, what percent of those that should have been labeled 0, were actually labeled 0?**\n",
    "\n",
    "Additionaly, we can see examples of what the model correctly and incorrectly labeled. \n",
    "\n",
    "For example the first tweet from the 'False negatives': '@SouthwestAir I will.make sure to tell everyone I know about this all around horrible experience...' is negative (sentiment score 0), but was predited as neutral (sentiment score 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and use the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the 'Test&Use' tab you can start using your model, hosted by Google. \n",
    "\n",
    "You just have to **input some positive or negative airline tweets in the text box and click predict**. \n",
    "\n",
    "Let's try this one: '@AmericanAir, I was waiting on the tarmac for two hours!'\n",
    "\n",
    "**AutoML Natural Language custom sentiment analysis model knows that losing your luggage is not fun and it gives a negative sentiment to this tweet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced dataset vs. balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for reference, we will make a comparison between a model trained on an unbalanced dataset, on the left side, and a balanced dataset on the right side. \n",
    "\n",
    "\n",
    "For the unbalanced dataset, the sentiment score 0 has three times more observations than sentiment score 1 and four times more obeservations that sentiment score 2.\n",
    " \n",
    "So, that's why our model predcits best for sentiment score 0. \n",
    "\n",
    "Additionally, we can see that we have significantly lower percentage for sentiment score 1. \n",
    "66% of the time, the model accurately predicted that tweets with the label 1, should have had label 1, \n",
    "wheres for the balanced dataset 75% of the time, the model accurately predicted that tweets with the label 1, should have had label 1.\n",
    "\n",
    "Overall, we can say that the model trained on the balanced dataset, on the right, performs better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![white%20%282%29.jpg](../images/24.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Section we will build a model for predicting the sentiment of airline tweets, using Python. Additionally, we will compare our model to the one built with Google AutoML Natural Language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = 1000  # Size of the validation set\n",
    "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 24  # Maximum number of words in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to remove English stopwords from a Pandas Series.'''\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "    \n",
    "'''Function to remove mentions, preceded by @, in a Pandas Series'''\n",
    "def remove_mentions(input_text):\n",
    "    return re.sub(r'@\\w+', '', input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the dataset we created in Section 5, 'airline_dataset_balanced.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/mkrsteska/Desktop/UNIL 2/Big-Scale Analytics/Projects/Project 1/data/airline_dataset_balanced.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Set column names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text', 'airline_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Clean the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Train-Test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Converting words to numbers**\n",
    "<br> In this step we assign a number to every word in the vocabulary. Then, we convert every tweet to a sequence of numbers by substituing every word with its corresponding number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Creating word sequences of equal length** <br>\n",
    "Before we can compute the word embeddings, we need to make sure the number sequences are of equal length. In the example below, we truncate sequences to length MAX_LEN, or pad them with zeroes to achieve this. First, we'll have a look at the length of the (cleaned) tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6182.000000\n",
       "mean       10.386768\n",
       "std         4.273308\n",
       "min         1.000000\n",
       "25%         7.000000\n",
       "50%        11.000000\n",
       "75%        14.000000\n",
       "max        24.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
    "seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the figures above we will set MAX_LEN to 24. This means we will not be truncating any words, only pad with zeros. This is to avoid losing information because the tweets are rather short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,  277,  118, 3900,  115,\n",
       "        149,   30], dtype=int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of padded sequence\n",
    "X_train_seq_trunc[10]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Converting the target classes to one-hot encodings** <br>\n",
    "The target classes are strings which need to be converted into numeric vectors. This is done with the LabelEncoder from Sklearn and the to_categorical method from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Splitting off validation data** <br>\n",
    "From the training data, we split off a validation set of 10% to use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Training word embeddings**\n",
    "\n",
    "Keras provides an Embedding layer which helps us to train specific word embeddings based on our training data. It will convert the words in our vocabulary to multi-dimensional vectors. The output of the embedding layer is flattened and connected to an output layer. The output layer has three neurons - one for each class. The values of the output neurons represent the probability that the tweet belongs to that class. The tweet is assigned to the class with highest probability.\n",
    "\n",
    "**Word Embedding** is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 24, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 579       \n",
      "=================================================================\n",
      "Total params: 80,579\n",
      "Trainable params: 80,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model = models.Sequential()\n",
    "emb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
    "emb_model.add(layers.Flatten())\n",
    "emb_model.add(layers.Dense(3, activation='softmax'))\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkrsteska/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5563 samples, validate on 619 samples\n",
      "Epoch 1/20\n",
      "5563/5563 [==============================] - 0s 27us/step - loss: 1.0932 - accuracy: 0.3683 - val_loss: 1.0880 - val_accuracy: 0.4103\n",
      "Epoch 2/20\n",
      "5563/5563 [==============================] - 0s 7us/step - loss: 1.0762 - accuracy: 0.5096 - val_loss: 1.0774 - val_accuracy: 0.4540\n",
      "Epoch 3/20\n",
      "5563/5563 [==============================] - 0s 9us/step - loss: 1.0583 - accuracy: 0.5833 - val_loss: 1.0646 - val_accuracy: 0.4879\n",
      "Epoch 4/20\n",
      "5563/5563 [==============================] - 0s 11us/step - loss: 1.0383 - accuracy: 0.5973 - val_loss: 1.0506 - val_accuracy: 0.5073\n",
      "Epoch 5/20\n",
      "5563/5563 [==============================] - 0s 14us/step - loss: 1.0169 - accuracy: 0.6061 - val_loss: 1.0363 - val_accuracy: 0.5121\n",
      "Epoch 6/20\n",
      "5563/5563 [==============================] - 0s 10us/step - loss: 0.9949 - accuracy: 0.6207 - val_loss: 1.0219 - val_accuracy: 0.5153\n",
      "Epoch 7/20\n",
      "5563/5563 [==============================] - 0s 9us/step - loss: 0.9725 - accuracy: 0.6301 - val_loss: 1.0076 - val_accuracy: 0.5331\n",
      "Epoch 8/20\n",
      "5563/5563 [==============================] - 0s 10us/step - loss: 0.9502 - accuracy: 0.6450 - val_loss: 0.9945 - val_accuracy: 0.5428\n",
      "Epoch 9/20\n",
      "5563/5563 [==============================] - 0s 10us/step - loss: 0.9281 - accuracy: 0.6556 - val_loss: 0.9817 - val_accuracy: 0.5460\n",
      "Epoch 10/20\n",
      "5563/5563 [==============================] - 0s 15us/step - loss: 0.9059 - accuracy: 0.6790 - val_loss: 0.9693 - val_accuracy: 0.5525\n",
      "Epoch 11/20\n",
      "5563/5563 [==============================] - 0s 13us/step - loss: 0.8829 - accuracy: 0.7034 - val_loss: 0.9571 - val_accuracy: 0.5687\n",
      "Epoch 12/20\n",
      "5563/5563 [==============================] - 0s 9us/step - loss: 0.8593 - accuracy: 0.7185 - val_loss: 0.9440 - val_accuracy: 0.5767\n",
      "Epoch 13/20\n",
      "5563/5563 [==============================] - 0s 10us/step - loss: 0.8349 - accuracy: 0.7478 - val_loss: 0.9305 - val_accuracy: 0.5848\n",
      "Epoch 14/20\n",
      "5563/5563 [==============================] - 0s 12us/step - loss: 0.8099 - accuracy: 0.7672 - val_loss: 0.9170 - val_accuracy: 0.6010\n",
      "Epoch 15/20\n",
      "5563/5563 [==============================] - 0s 10us/step - loss: 0.7846 - accuracy: 0.7839 - val_loss: 0.9033 - val_accuracy: 0.6058\n",
      "Epoch 16/20\n",
      "5563/5563 [==============================] - 0s 10us/step - loss: 0.7592 - accuracy: 0.7980 - val_loss: 0.8899 - val_accuracy: 0.6187\n",
      "Epoch 17/20\n",
      "5563/5563 [==============================] - 0s 9us/step - loss: 0.7336 - accuracy: 0.8170 - val_loss: 0.8766 - val_accuracy: 0.6317\n",
      "Epoch 18/20\n",
      "5563/5563 [==============================] - 0s 9us/step - loss: 0.7079 - accuracy: 0.8269 - val_loss: 0.8633 - val_accuracy: 0.6478\n",
      "Epoch 19/20\n",
      "5563/5563 [==============================] - 0s 9us/step - loss: 0.6823 - accuracy: 0.8406 - val_loss: 0.8500 - val_accuracy: 0.6607\n",
      "Epoch 20/20\n",
      "5563/5563 [==============================] - 0s 12us/step - loss: 0.6573 - accuracy: 0.8508 - val_loss: 0.8372 - val_accuracy: 0.6640\n"
     ]
    }
   ],
   "source": [
    "emb_model.compile(optimizer = 'rmsprop', \n",
    "                  loss = 'categorical_crossentropy', \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "emb_history = emb_model.fit(X_train_emb, \n",
    "                            y_train_emb, \n",
    "                            epochs = NB_START_EPOCHS,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            validation_data = (X_valid_emb, y_valid_emb),\n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have a validation accuracy of 66.40%. The number of words in the tweets is quite low, so this result is rather good.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687/687 [==============================] - 0s 28us/step\n",
      "Test accuracy of word embeddings model: 70.16%\n"
     ]
    }
   ],
   "source": [
    "emb_model.fit(X_train_seq_trunc\n",
    "              , y_train_oh\n",
    "              , epochs=6\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "\n",
    "emb_results = emb_model.evaluate(X_test_seq_trunc, y_test_oh)\n",
    "    \n",
    "print('Test accuracy of word embeddings model: {0:.2f}%'.format(emb_results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the model on all data (including the validation data, but excluding the test data) and set the number of epochs to 6, we get a **test accuracy of 70.16%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Get the predicted values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_le = emb_model.predict_classes(X_test_seq_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Classification report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classification report below we can see the precision and recall of our model - 'macro avg'. \n",
    "\n",
    "Additionally, we can see the precision and recall broken down for each category. \n",
    "\n",
    "If we take label 0:\n",
    "- **Precision tells us of tweets that our model labeled  0, how many really were 0?**\n",
    "\n",
    "- **Recall, on the other hand, tells us of all the tweets in the dataset, what percent of those that should have been labeled 0, were actually labeled 0?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.721     0.713     0.717       247\n",
      "           1      0.648     0.671     0.659       225\n",
      "           2      0.738     0.721     0.729       215\n",
      "\n",
      "    accuracy                          0.702       687\n",
      "   macro avg      0.702     0.702     0.702       687\n",
      "weighted avg      0.703     0.702     0.702       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test_le, y_pred_le, labels=[0, 1, 2], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confustion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Confusion Matrix tells us how the real labels of the dataset compare to the labels that our model predicted. \n",
    "\n",
    "For example, 71% of the time, the model accurately predicted that tweets with the label 0, should have had label 0, but 19% of the time, it also thought that tweets with label 1, those that are just neutral, should be labeled 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712551</td>\n",
       "      <td>0.190283</td>\n",
       "      <td>0.097166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.191111</td>\n",
       "      <td>0.671111</td>\n",
       "      <td>0.137778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.720930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.355167</td>\n",
       "      <td>0.339156</td>\n",
       "      <td>0.305677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted         0         1         2\n",
       "True                                   \n",
       "0          0.712551  0.190283  0.097166\n",
       "1          0.191111  0.671111  0.137778\n",
       "2          0.116279  0.162791  0.720930\n",
       "All        0.355167  0.339156  0.305677"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test_le, y_pred_le, rownames=['True'], colnames=['Predicted'], margins=True, normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the precision and recall for each class of both models.  \n",
    "\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Class </th>\n",
    "    <th colspan=\"2\">Our Model</th>\n",
    "    <th colspan=\"2\">Google AutoML model</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Precision</th>\n",
    "    <th>Recall</th>\n",
    "    <th>Precision</th>\n",
    "    <th>Recall</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Sentiment score 0</th>\n",
    "    <td>72.1%</td>\n",
    "    <td>71.3%</td>\n",
    "    <td>77.27%</td>\n",
    "    <td> 81.66%</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Sentiment score 1</th>\n",
    "    <td> 64.8%</td>\n",
    "    <td> 67.1%</td>\n",
    "    <td> 75.11%</td>\n",
    "    <td> 75.11%</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Sentiment score 2</th>\n",
    "    <td> 73.8%</td>\n",
    "    <td> 72.1%</td>\n",
    "    <td> 87.69%</td>\n",
    "    <td> 82.79%</td>\n",
    "\n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "  \n",
    "We come to conclusion that the Google AutoML custom sentiment analysis outperforms our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explained what sentiment analysis is and we listed the main approaches and applications of sentiment analysis. Moreover, we built an AutoML Natural Language custom sentiment analysis model that can predict the sentiment of airline tweets. Additionally, we built a sentiment analysis model using Python.\n",
    "\n",
    "Google Cloud AutoML offers a wide range of products that can be used across many fields. It requres limited machine learning expertise and is very intuituve and easy to use as a platform. \n",
    "\n",
    "Specifically, building a custom sentiment analysis model using Google AutoML Natural Language was very pleasent experience. We did not encounter any difficulties in understanding the flow of the entire process. The UI is very readable, even for people who are not specilized in the field of machine learning. The results we got, with a rather small dataset, were excellent. The only problem we faced was when we were uploading the dataset. We realised through trial and error what are the requirements for the dataset. We did some data preparation in Python. However, there is a posibility to prepare the data in an excel sheet, which is more intuitive way to prepare the data. \n",
    "\n",
    "On the other hand, building a sentiment analysis model using Keras and Word Embedding Layer from Keras was not so straightforward task. We found a blog post that implements the model, but adapting it to our needs, doing some small modifications and understanding the process took significantly more time and browsing the internet. \n",
    "\n",
    "We would highly recommend using Google AutoML Natural Language for building a custom sentiment model. It does not require expertise in machine learning and coding. After building your own model, you can use it for your own business, and gain some very valuable insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sentiment Analysis](https://monkeylearn.com/sentiment-analysis/) <br>\n",
    "[Sentiment Analysis Explained](https://www.lexalytics.com/technology/sentiment-analysis)<br>\n",
    "[Understandind Sentiment Analysis](https://www.brandwatch.com/blog/understanding-sentiment-analysis/)<br>\n",
    "[Analyzing sentiment of text with domain-specific vocabulary and topics](https://medium.com/google-cloud/analyzing-sentiment-of-text-with-domain-specific-vocabulary-and-topics-726b8f287aef) <br>\n",
    "[Making sentiment predictions with AutoML from Google Sheets](https://medium.com/google-cloud/analyzing-sentiment-of-text-with-domain-specific-vocabulary-and-topics-bdcca7ac8c3e)<br>\n",
    "[Using Word Embeddings for Sentiment Analysis](https://towardsdatascience.com/word-embeddings-for-sentiment-analysis-65f42ea5d26e)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
